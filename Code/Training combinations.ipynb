{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import kendalltau\n",
    "import os.path\n",
    "import winsound\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import loguniform\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import os.path\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.svm import SVC\n",
    "from pandas_profiling import ProfileReport\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from scipy.stats import pointbiserialr\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.cluster import KMeans\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from pandas_profiling import ProfileReport\n",
    "from scipy.stats import f_oneway, shapiro\n",
    "from io import BytesIO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import dtale\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "import matplotlib.cm\n",
    "import re\n",
    "import base64\n",
    "from scipy.stats import pointbiserialr, pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import category_encoders as ce\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "import winsound\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "np.set_printoptions(threshold=sys.maxsize, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = {'business_area': str, 'sex': str, 'employment_status': str,\n",
    "            'time': str, 'time_ESAW': str, 'severity': str, 'enterprise_size': str,\n",
    "            'citizenship': str, 'profession_code': str, 'type_of_injury': str\n",
    "            , 'injured_bodypart': str, 'workstation': str,\n",
    "            'working_environment': str, 'working_process': str\n",
    "            , 'specific_physical_activity': str,\n",
    "            'material_agent_of_physical_act.': str,\n",
    "            'deviation': str, 'material_agent_of_deviation': str\n",
    "            , 'contact_mode_of_injury': str, 'material_agent_of_contact_mode': str, 'general_profession_code': str, 'month': str, 'weekofyear': str, 'dayofweek': str, 'material_agent_of_physical_act': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducing results use data.csv in your working directory and comment the fist line out\n",
    "os.chdir('C:/Users/Mario/OneDrive - Tartu Ãœlikool/IT_mitteinformaatikutele_MSc/Kevad 2023/Student_Project_Contest_UT_2023/Code')\n",
    "df = pd.read_csv(\"data.csv\", encoding='latin-1', dtype = datatypes, index_col=0)\n",
    "df = df.drop(columns= ['date', 'time', 'time_ESAW', 'datetime', 'lost_days', 'type_of_injury', 'injured_bodypart', 'severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = list(df.business_sector.unique())\n",
    "classes = list(df.general_profession_class.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin of error: PASS for C9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-5e2506b37d41>\", line 154, in <module>\n",
      "    smote = SMOTE(random_state=rs)\n",
      "NameError: name 'rs' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"c:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\ntpath.py\", line 647, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5e2506b37d41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0msmote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rs' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2047\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1435\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1436\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1193\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "dff = df.copy()\n",
    "for f in sectors:\n",
    "\n",
    "    for e in classes:\n",
    "        \n",
    "        df = dff.copy()\n",
    "        df = df[(df['business_sector'] == f)]\n",
    "        df = df[(df['general_profession_class'] == e)]\n",
    "        if (df['target'] == 1).sum() < 5:\n",
    "            print('Not enough positive samples for sector: ' + f + ' and class: ' + str(e))\n",
    "            continue\n",
    "        df_sub = df.drop(columns=['business_sector', 'general_profession_class'])\n",
    "\n",
    "        df_sub_copy = df_sub.copy()\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        #TRAINING ON SIGNIFICANT FEATURES\n",
    "        # --------------------------------------------------------------------------------\n",
    "\n",
    "        def encode_categorical(df, categorical_columns):\n",
    "            le = LabelEncoder()\n",
    "            for col in categorical_columns:\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "            return df\n",
    "\n",
    "        categorical_columns = df_sub.select_dtypes(include='object').columns.tolist()\n",
    "        df_encoded = encode_categorical(df_sub, categorical_columns)\n",
    "\n",
    "        # Get the column names with 'int32' and 'int64' dtypes\n",
    "        int32_columns = df_encoded.select_dtypes(include='int32').columns.tolist()\n",
    "        int64_columns = df_encoded.select_dtypes(include='int64').columns.tolist()\n",
    "\n",
    "        # Combine the two lists to create the categorical_columns list\n",
    "        categorical_columns = int32_columns + int64_columns\n",
    "\n",
    "        def chi_squared_test(df, target, categorical_columns):\n",
    "            significant_features = []\n",
    "            for col in categorical_columns:\n",
    "                contingency_table = pd.crosstab(df[col], df[target])\n",
    "                chi2, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significant_features.append(col)\n",
    "            \n",
    "            return significant_features\n",
    "\n",
    "        significant_chi2_features = chi_squared_test(df_encoded, 'target', categorical_columns)\n",
    "\n",
    "\n",
    "        def kendall_correlation(df, target, numerical_columns):\n",
    "            significant_features = []\n",
    "            for col in numerical_columns:\n",
    "                kendall_tau, p_value = stats.kendalltau(df[col], df[target])\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significant_features.append(col)\n",
    "            \n",
    "            return significant_features\n",
    "\n",
    "        numerical_columns = df_sub.select_dtypes(include='float64').columns.tolist()\n",
    "        significant_kendall_features = kendall_correlation(df_sub, 'target', numerical_columns)\n",
    "\n",
    "        significant_features = significant_chi2_features + significant_kendall_features\n",
    "\n",
    "        df = df_sub_copy[significant_features]\n",
    "\n",
    "        # Encode categorical features with no order\n",
    "        all_object_cols = list(df.select_dtypes(include=['object']).columns)\n",
    "        df = pd.get_dummies(df, columns=all_object_cols)\n",
    "\n",
    "        # Separate independent variables and dependent variable for train set\n",
    "        X = df.drop(columns=['target'])\n",
    "        y = df['target']\n",
    "\n",
    "        # Balance the target class using random over-sampling\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        try:\n",
    "            float_cols = X_train.select_dtypes(include='float64').columns.tolist()\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            X_train[float_cols] = scaler.fit_transform(X_train[float_cols])\n",
    "            X_test[float_cols] = scaler.transform(X_test[float_cols])\n",
    "        except ValueError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "        feat = 'significant_features'\n",
    "\n",
    "        # ----------------------------------------------------------------------------\n",
    "        #TRAINING ON ALL FEATURES\n",
    "        # ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        # Encode categorical features with no order\n",
    "        \"\"\" all_object_cols = list(df.select_dtypes(include=['object']).columns)\n",
    "        df = pd.get_dummies(df, columns=all_object_cols)\n",
    "\n",
    "        # Separate independent variables and dependent variable for train set\n",
    "        X = df.drop(columns=['target'])\n",
    "        y = df['target']\n",
    "\n",
    "        # Balance the target class using random over-sampling\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Scale float64 variables\n",
    "        float_cols = X_train.select_dtypes(include='float64').columns.tolist()\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        X_train[float_cols] = scaler.fit_transform(X_train[float_cols])\n",
    "        X_test[float_cols] = scaler.transform(X_test[float_cols])\n",
    "\n",
    "\n",
    "        feat = 'all_features' \"\"\"\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        nr_of_independent_var = len(X_train.columns)\n",
    "        independent_var_names = [col for col in X_train.columns if col != 'target']\n",
    "        target_0 = y_train.value_counts()[0]\n",
    "        target_1 = y_train.value_counts()[1]\n",
    "\n",
    "        # Calculate the proportion of positive target class in the y_test data\n",
    "        positive_class_count = y_test.sum()\n",
    "        total_count = len(y_test)\n",
    "        proportion = positive_class_count / total_count\n",
    "\n",
    "        # Calculate the 95% confidence interval for the proportion\n",
    "        z = stats.norm.ppf(0.975)  # 95% confidence level\n",
    "        margin_of_error = z * np.sqrt((proportion * (1 - proportion)) / total_count)\n",
    "        confidence_interval = (proportion - margin_of_error, proportion + margin_of_error)\n",
    "\n",
    "        # Define a threshold for an acceptable margin of error (you can adjust this value)\n",
    "        acceptable_margin_of_error = 0.1\n",
    "\n",
    "        # Print evaluation sentence\n",
    "        if margin_of_error <= acceptable_margin_of_error:\n",
    "            print(f\"Margin of error: PASS for {f}{e}\")\n",
    "        else:\n",
    "            print(f\"Margin of error: FAIL for {f}{e}\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "        # Use selected resampling technique\n",
    "        \n",
    "        \"\"\" over_sampler = RandomOverSampler(random_state=42)\n",
    "        X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n",
    "        balancing_tech = 'ROS' \"\"\"\n",
    "        \n",
    "        \"\"\" under_sampler = RandomUnderSampler(random_state=42)\n",
    "        X_train, y_train = under_sampler.fit_resample(X_train, y_train)\n",
    "        balancing_tech = 'RUS' \"\"\"\n",
    "\n",
    "        try:\n",
    "            smote = SMOTE(random_state=rs)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            balancing_tech = 'SMOTE'\n",
    "        except ValueError as e:\n",
    "            print(f\"An error occurred in SMOTE balancing: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "          # Check if the Excel file already exists\n",
    "        file_exists = os.path.isfile(f'combinations_{feat}_{balancing_tech}.xlsx')\n",
    "\n",
    "        # If the file doesn't exist, create a new DataFrame to store the results\n",
    "        if not file_exists:\n",
    "            results_df = pd.DataFrame(columns=[\n",
    "                \"Sector\",\n",
    "                \"Class\",\n",
    "                \"Nr_of_ind._var\",\n",
    "                \"Names_of_ind._var\",\n",
    "                \"Target_0_count\",\n",
    "                \"Target_1_count\",\n",
    "                \"Model_name\",\n",
    "                \"Balancing_tech.\",\n",
    "                \"AUCROC\",\n",
    "                \"CV_AUCROC\",\n",
    "                \"CV_AUCROC_std\",\n",
    "                \"AVG_F1\",\n",
    "                \"F1_score_0\",\n",
    "                \"F1_score_1\",\n",
    "                \"Precision_0\",\n",
    "                \"Precision_1\",\n",
    "                \"Recall_0\",\n",
    "                \"Recall_1\",\n",
    "            ])\n",
    "        else:\n",
    "            # If the file exists, load the existing DataFrame from the file\n",
    "            results_df = pd.read_excel(f'combinations_{feat}_{balancing_tech}.xlsx')\n",
    "\n",
    "        rs = 42\n",
    "        rf_model = RandomForestClassifier(random_state=rs)\n",
    "        lgbm_model = LGBMClassifier(random_state=rs)\n",
    "        svm_model = SVC(kernel='rbf', probability=True, random_state=rs)\n",
    "        xgb_model = xgb.XGBClassifier(random_state=rs)\n",
    "        lr_model = LogisticRegression(max_iter=1000, random_state=rs)\n",
    "\n",
    "        models = [rf_model, lgbm_model, xgb_model, svm_model, lr_model]\n",
    "        model_names = ['Random Forest', 'LightGBM', 'XGBoost', 'SVM', 'Logistic Regression']\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)   \n",
    "\n",
    "        for k, model in enumerate(models):\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "            try:\n",
    "                auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping {model_names[k]} for {f}{e} due to only one class present in y_test\")\n",
    "                continue\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred)\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "            f1_class_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "            f1_class_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "            avg_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            precision_class_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "            precision_class_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "            recall_class_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "            recall_class_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "            try:\n",
    "                balancing_tech = balancing_tech\n",
    "            except NameError:\n",
    "                balancing_tech = None\n",
    "            try:\n",
    "                comment1 = comment1\n",
    "            except NameError:\n",
    "                comment1 = None\n",
    "            # Create a dictionary of results\n",
    "            result = {\n",
    "                \"Sector\": f,\n",
    "                \"Class\": e,\n",
    "                \"Nr_of_ind._var\": nr_of_independent_var,\n",
    "                \"Names_of_ind._var\": \", \".join(independent_var_names),\n",
    "                \"Target_0_count\": target_0,\n",
    "                \"Target_1_count\": target_1,\n",
    "                \"Model_name\": model_names[k],\n",
    "                \"Balancing_tech.\": balancing_tech,\n",
    "                \"AUCROC\": auc_roc,\n",
    "                \"CV_AUCROC\": np.mean(cv_scores),\n",
    "                \"CV_AUCROC_std\": np.std(cv_scores),\n",
    "                \"AVG_F1\": avg_f1,\n",
    "                \"F1_score_0\": f1_class_0,\n",
    "                \"F1_score_1\": f1_class_1,\n",
    "                \"Precision_0\": precision_class_0,\n",
    "                \"Precision_1\": precision_class_1,\n",
    "                \"Recall_0\": recall_class_0,\n",
    "                \"Recall_1\": recall_class_1\n",
    "            }\n",
    "\n",
    "            # Append the result to the results DataFrame\n",
    "            results_df = pd.concat([results_df, pd.DataFrame(result, index=[0])], ignore_index=True)\n",
    "            print(f\"Salvestasin excelisse: {f}{e}\")\n",
    "            # Save the results DataFrame to the Excel file\n",
    "            results_df.to_excel(f'combinations_{feat}_{balancing_tech}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

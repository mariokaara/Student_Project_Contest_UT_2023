{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"MvC6STSfxSSO"},"outputs":[],"source":["import os\n","import numpy as np\n","import math\n","import sys\n","import pandas as pd\n","from sklearn.impute import KNNImputer\n","import pytz\n","import seaborn as sns\n","import csv\n","from scipy.stats import skew, boxcox\n","from math import log, sqrt\n","import pickle\n","import datetime\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from scipy import stats\n","import re\n","plt.rcParams['figure.facecolor'] = 'white'"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-15-4799ce74be0c>:5: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  pd.set_option('display.max_colwidth', -1) # to display the full contents of each column\n"]}],"source":["np.set_printoptions(threshold=sys.maxsize, suppress=True)\n","pd.set_option('display.max_rows', None) # to show all rows in a Series\n","pd.set_option('display.max_columns', None) # to show all columns in a DataFrame\n","pd.set_option('display.width', None) # to let pandas set the display width\n","pd.set_option('display.max_colwidth', -1) # to display the full contents of each column"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"OA8w_cZMxSSQ"},"outputs":[],"source":["os.chdir('C:/Users/Mario/OneDrive - Tartu Ülikool/IT_mitteinformaatikutele_MSc/Kevad 2023/Student_Project_Contest_UT_2023/Code')\n","df = pd.read_csv(\"2002-2022_encoded_csv.csv\", encoding='cp1252')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["FIGURE_PATH = os.path.join(os.getcwd(), \"figures\")\n","if not os.path.exists(FIGURE_PATH):\n","    os.makedirs(FIGURE_PATH)\n","plt.rcParams['savefig.directory'] = FIGURE_PATH"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Translating the names of the columns to english\n","df.rename({'liiklusonnetus': 'is_traffic_accident', 'toovoimetuspaevi': 'lost_days', 'kokkupuute_vigastuse_laadi_materiaalne_mojur': 'material_agent_of_contact_mode', 'kokkupuude_vigastuse_laad': 'contact_mode_of_injury', 'korvalekalde_materiaalne_mojur': 'material_agent_of_deviation', 'korvalekalle': 'deviation', 'tegevuse_materiaalne_mojur': 'material_agent_of_physical_act.', 'konkreetne_fuusiline_tegevus': 'specific_physical_activity', 'tooprotsess': 'working_process', 'tookeskkond': 'working_environment', 'tootamiskoht': 'workstation', 'vigastatud_kehaosa': 'injured_bodypart', 'vigastuse_liik': 'type_of_injury', 'ettevotte_tootajate_arv_kood': 'enterprise_size', 'vanusegrupp': 'age_group', 'otsus_uurimiseks': 'under_investigation', 'Tooandja ID': 'enterprise_ID', 'Isiku ID': 'employee_ID','ametikood': 'profession_code', 'tooalane_seisund': 'employment_status', 'pohjus': 'causes', 'pohjused_UKV': 'causes_verified', 'ettevotte_tootajate_arv': 'employees_in_enterprise', 'tootajate_arv_struktuuriyksuses': 'employees_in_structural_unit', 'onnetuse_tegevusala': 'business_area', 'sugu': 'sex', 'vanus': 'age', 'toostaa_selles_ametis_selle_tooandja_juures': 'employment_years', 'onnetuse_kellaaeg': 'time', 'eestisse_lahetatud_tootaja': 'is_posted_worker', 'onnetuse_kuupaev': 'date', 'taistunnid_paeva_vahetuse_algusest': 'full_hours_from_startofwork', 'raskusaste': 'severity', 'rap_tookeskk_riskianalyys_tehtud': 'is_risk_assessment_done', 'rap_kas_riskid_arvestatud':  'are_risks_considered', 'kodakondsus': 'citizenship', 'toimumiskoht': 'location'}, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Clean dataset\n","cols = df.columns\n","\n","for i in cols:\n","    df.loc[df[i] == ' -- ', i] = np.nan\n","    df.loc[df[i] == ',, -- ', i] = np.nan\n","    df.loc[df[i] == ' ', i] = np.nan \n","    df.loc[df[i] == '09-May', i] = np.nan "]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# select the rows where at least 8 features are missing\n","rows_with_14_nan = df[df.isna().sum(axis=1) >= 8]\n","\n","# drop the rows with at least 8 NaN values from the dataframe\n","df = df.drop(rows_with_14_nan.index)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df = df.drop(columns=['enterprise_ID', 'employee_ID', 'under_investigation', 'causes_verified', 'is_traffic_accident', 'is_posted_worker', 'employees_in_structural_unit', 'is_risk_assessment_done', 'are_risks_considered'])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def enterprise_size(string):\n","    if type(string) != float: \n","        code = string.split(' -- ')\n","        if code[0] == '':\n","            try:\n","                if code[1][0] == '0':\n","                    return '0'\n","            except:\n","                return np.nan\n","        elif code[0] == '9':\n","            return np.nan\n","        else:\n","            return code[0]\n","    else:\n","        return np.nan\n","\n","\n","df['enterprise_size'] = df['enterprise_size'].apply(lambda x: enterprise_size(x))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['enterprise_size'])"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["ordinal_mapping = {\n","    '1': 1,\n","    '2': 2,\n","    '3': 3,\n","    '4': 4,\n","    '5': 5,\n","    '0': 0\n","}\n","\n","df['enterprise_size_ordinal_enc'] = df['enterprise_size'].map(ordinal_mapping)\n","df['enterprise_size_ordinal_enc'] = df['enterprise_size_ordinal_enc'].astype('float64')\n","df.drop(columns=['enterprise_size', 'employees_in_enterprise'], inplace=True)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Function to consolidate the sex codes\n","def business_area(string):\n","    if type(string) != float: \n","        code = string.split(' -- ')\n","        if code[0] == '':\n","            return '00'\n","        else:\n","            return code[0][0:2]\n","    else:\n","        return '00'\n","\n","\n","df['business_area'] = df['business_area'].apply(lambda x: business_area(x))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def add_general_business_code(df1, df2):\n","    # replace non-numeric values in 'business_area' column with NaN\n","    df1['business_area'] = pd.to_numeric(df1['business_area'], errors='coerce')\n","    \n","    # create a dictionary mapping business area codes to general business codes\n","    code_map = df2.set_index('Kood')['Üldine_tähis'].to_dict()\n","    \n","    # add a new 'General_business_code' column to df1, initially filled with NaN\n","    df1['business_sector'] = pd.Series(dtype='float64')\n","    \n","    # iterate over the rows in df1, checking if the business area code is in the dictionary\n","    for i, row in df1.iterrows():\n","        business_area_code = row['business_area']\n","        if np.isnan(business_area_code):\n","            general_code = None\n","        elif business_area_code in code_map:\n","            general_code = code_map[business_area_code]\n","        else:\n","            general_code = None\n","        df1.at[i, 'business_sector'] = general_code\n","    \n","    return df1\n","\n","df_csv = pd.read_csv('emtak.csv', sep=';')\n","df = add_general_business_code(df, df_csv)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Function to consolidate the sex codes\n","def sex(string):\n","    if type(string) != float: \n","        code = string.split(' -- ')\n","        if code[0] == '':\n","            return '9'\n","        else:\n","            return code[0]\n","    else:\n","        return '9'\n","\n","\n","df['sex'] = df['sex'].apply(lambda x: sex(x))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["ordinal_mapping = {\n","    '010 -- 0-17': 1,\n","    '1 -- 18-24': 2,\n","    '2 -- 25-34': 3,\n","    '3 -- 35-44': 4,\n","    '4 -- 45-54': 5,\n","    '5 -- 55-64': 6,\n","    '6 -- 65 või vanem': 7\n","\n","}\n","\n","df['age_group'] = df['age_group'].map(ordinal_mapping)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["df = df.drop(columns=['age_group'])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Function to consolidate the sex codes\n","def employment_status(string):\n","    if type(string) != float: \n","        code = string.split(' -- ')\n","        if code[0] == '':\n","            return '000'\n","        else:\n","            return code[0]\n","    else:\n","        return '000'\n","\n","\n","df['employment_status'] = df['employment_status'].apply(lambda x: employment_status(x))"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["df = df[df['employment_status'] != '000']\n","df.loc[df['employment_years'] > 2000, 'employment_years'] = np.nan\n","subset = df.loc[(df['employment_years'] > 0) & (df['employment_years'] < 1), 'employment_years']\n","median = subset.median()\n","df['employment_years'] = df['employment_years'].replace(0, median)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["df.dropna(subset='time', inplace=True)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def fix_time(time_str):\n","    if len(time_str) < 4:\n","        time_str = '00:00'\n","    else:\n","        time_str = time_str[0:2]+':00'\n","    return time_str\n","\n","df['time'] = df['time'].apply(lambda x: fix_time(x))\n","df['datetime'] = df['date'] + 'T' + df['time']"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# define the input format for the string datetime values\n","input_format = '%d-%m-%yT%H:%M'\n","\n","# create a function to convert datetime strings to UTC timezone\n","def convert_to_utc(string_datetime):\n","    # parse the string datetime into a datetime object\n","    dt = datetime.datetime.strptime(string_datetime, input_format)\n","    \n","    # create a timezone object for the local time zone\n","    local_tz = pytz.timezone('Europe/Istanbul') # for UTC+3\n","    \n","    # set the timezone of the datetime object to the local time zone\n","    dt_local = local_tz.localize(dt)\n","    \n","    # convert the datetime object to UTC timezone\n","    utc_tz = pytz.utc\n","    dt_utc = dt_local.astimezone(utc_tz)\n","    \n","    # format the UTC datetime as a string\n","    output_format = '%Y-%m-%dT%H:%M:%S.%fZ'\n","    return dt_utc.strftime(output_format)\n","\n","# apply the convert_to_utc function to the datetime column in your DataFrame\n","df['datetime'] = df['datetime'].apply(convert_to_utc)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["df['datetime'] = pd.to_datetime(df['datetime']).apply(lambda x: x.strftime('%Y-%m-%dT%H:%M'))"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["def create_time_features(df, date_column_name):\n","    # Check if date_column_name is a datetime-like object\n","    if not pd.api.types.is_datetime64_dtype(df[date_column_name]):\n","        df[date_column_name] = pd.to_datetime(df[date_column_name], errors='coerce')\n","        if not pd.api.types.is_datetime64_dtype(df[date_column_name]):\n","            raise ValueError(\"Column {} cannot be converted to datetime.\".format(date_column_name))\n","\n","    # Check for missing or invalid values in the date_column_name column\n","    if df[date_column_name].isnull().values.any():\n","        raise ValueError(\"Column {} contains missing values.\".format(date_column_name))\n","    \n","    # Create time features\n","    df = df.copy()\n","    df['dayofweek'] = (df[date_column_name].dt.dayofweek+1).astype(str) # Day of week 1..7\n","    df['month'] = df[date_column_name].dt.month.astype(str) # Month 1..12\n","    return df\n","\n","df = create_time_features(df, 'date')"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['time'])"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def time(x):\n","    if ':' not in x:\n","        return '99'\n","    try:\n","        s = x.split(':')\n","        if s[0] == '':\n","            return '00'\n","        else:\n","            return s[0]\n","    except:\n","        return '99'\n","    \n","df['time_ESAW'] = df['time'].apply(lambda x: time(x))\n","\n","# Convert 'time' to numeric format\n","df['time_int'] = df['time_ESAW'].astype(int)\n","\n","# Convert 'time' to 'sin_time' and 'cos_time'\n","df['sin_time'] = np.sin(2*np.pi*df['time_int']/24)\n","df['cos_time'] = np.cos(2*np.pi*df['time_int']/24)\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["df = df.drop(columns='time_int')"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["df['is_business_hour'] = df['time'].apply(lambda x: 1 if '08' <= x <= '16' else 0)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['full_hours_from_startofwork'])"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["constant = 0.5\n","df['full_hours_from_startofwork'] = df['full_hours_from_startofwork'].replace(0, constant)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# Calculate Q1 and Q3\n","Q1 = df['full_hours_from_startofwork'].quantile(0.25)\n","Q3 = df['full_hours_from_startofwork'].quantile(0.75)\n","\n","# Calculate IQR\n","IQR = Q3 - Q1\n","\n","# Define upper outlier limit\n","upper_limit = Q3 + 1.5 * IQR\n","\n","# Filter dataframe to keep only rows below upper limit\n","df = df[df['full_hours_from_startofwork'] <= upper_limit]"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Skewness of the feature:  0.5173817300663335\n","Skewness of log-transformed feature:  -0.6641677252336851\n","Skewness of sqrt-transformed feature:  -0.06930141460471301\n","Skewness of box-cox-transformed feature:  -0.1512248475487352\n","Square root transformation resulted in the lowest skewness.\n"]}],"source":["# assuming your feature is a pandas series\n","feature = df.full_hours_from_startofwork\n","\n","# removing zero values if any\n","feature = feature.replace(0, np.nan).dropna()\n","\n","# measuring skewness\n","skewness = skew(feature)\n","\n","print(\"Skewness of the feature: \", skewness)\n","\n","# log transform\n","log_feature = feature.apply(np.log)\n","\n","# square root transform\n","sqrt_feature = feature.apply(np.sqrt)\n","\n","# box-cox transform\n","boxcox_feature, _ = boxcox(feature)\n","\n","# comparing the skewness of the transformed features\n","print(\"Skewness of log-transformed feature: \", skew(log_feature))\n","print(\"Skewness of sqrt-transformed feature: \", skew(sqrt_feature))\n","print(\"Skewness of box-cox-transformed feature: \", skew(boxcox_feature))\n","\n","# choosing the best transformation based on the skewness\n","if abs(skew(log_feature)) < abs(skew(sqrt_feature)) and abs(skew(log_feature)) < abs(skew(boxcox_feature)):\n","    transformed_feature = log_feature\n","    print(\"Log transformation resulted in the lowest skewness.\")\n","elif abs(skew(sqrt_feature)) < abs(skew(boxcox_feature)):\n","    transformed_feature = sqrt_feature\n","    print(\"Square root transformation resulted in the lowest skewness.\")\n","else:\n","    transformed_feature = boxcox_feature\n","    print(\"Box-Cox transformation resulted in the lowest skewness.\")"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["df['full_hours_from_startofwork'] = transformed_feature"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def add_target_feature(df):\n","    df['target'] = df['severity'].apply(lambda x: 1 if (x == '22 -- surm' or x == '20 -- raske') else 0)\n","    # df = df.drop(columns='severity')\n","    return df\n","\n","df = add_target_feature(df)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["def transform_location(location):\n","    if pd.isna(location) or (type(location) == str and location.strip() == \"\") or location == 'Määramata':\n","        return np.nan\n","    elif type(location) == str and len(location.split()) >= 2 and location.split()[1] == 'mk':\n","        return location.split()[0] + ' county'\n","    elif location == 'Tallinn':\n","        return 'Harju county'\n","    else:\n","        return 'Foreign country'\n","\n","df['location'] = df['location'].apply(lambda x: transform_location(x))"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['location'])"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["new_to_old_dict = {\n","    'Harjumaa': 'Harju county',\n","    'Hiiumaa': 'Hiiu county',\n","    'Ida-Virumaa': 'Ida-Viru county',\n","    'Jarvamaa': 'Järva county',\n","    'Jogevamaa': 'Jõgeva county',\n","    'Laane-Virumaa': 'Lääne-Viru county',\n","    'Laanemaa': 'Lääne county',\n","    'Parnumaa': 'Pärnu county',\n","    'Polvamaa': 'Põlva county',\n","    'Raplamaa': 'Rapla county',\n","    'Saaremaa': 'Saare county',\n","    'Tartumaa': 'Tartu county',\n","    'Valgamaa': 'Valga county',\n","    'Viljandimaa': 'Viljandi county',\n","    'Vorumaa': 'Võru county',\n","    'Foreign country': 'Foreign country'\n","}\n","\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["old_to_new_dict = {value: key for key, value in new_to_old_dict.items()}\n","df['location'] = df['location'].replace(old_to_new_dict)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%dT%H:%M')\n","df['datetime'] = df['datetime'].apply(lambda x: x.timestamp())"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["with open('data.pickle', 'rb') as f:\n","    data = pickle.load(f)\n","\n","df['temperature'] = 0\n","df['rain'] = 0\n","df['snowfall'] = 0\n","\n","for f, item in enumerate(list(data.keys())):\n","    my_dict = {}\n","    for key, value in data[item].items():\n","        if pd.isna(key):\n","            continue\n","        timestamp = key.timestamp()\n","        my_dict[timestamp] = value\n","\n","    for i, row in df.loc[df['location'] == list(df.location.unique())[f]].iterrows():\n","\n","        datetime_value = row['datetime']\n","        \n","        if datetime_value in my_dict:\n","            # get the list of items corresponding to the key\n","            items_list = my_dict[datetime_value]\n","            \n","            # set the values of the new columns in df to the corresponding items in the list\n","            if len(items_list) == 5:\n","                df.at[i, 'temperature'] = items_list[0]\n","                df.at[i, 'rain'] = items_list[1]\n","                df.at[i, 'snowfall'] = items_list[2]\n","            else:\n","                # handle unexpected list length as desired, e.g. skip or replace with default values\n","                continue\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# Replace zero values with np.nan for specified rows\n","df.loc[df['location'] == 'Foreign country', ['temperature', 'rain', 'snowfall']] = df.loc[df['location'] == 'Foreign country', ['temperature', 'rain', 'snowfall']].replace(0, np.nan)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['temperature', 'rain', 'snowfall'])"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# convert string values to numeric values\n","df['temperature'] = pd.to_numeric(df['temperature'])\n","df['rain'] = pd.to_numeric(df['rain'])\n","df['snowfall'] = pd.to_numeric(df['snowfall'])"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["df['rain'] = np.where(df['rain'] == 0, 0, 1)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["df['snowfall'] = np.where(df['snowfall'] == 0, 0, 1)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['causes'])"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def cause_list(string):\n","  if type(string) != float:\n","    m = string.split(\" -- \")[0]\n","    if ',' in m:\n","      cause_list = []\n","      for i in m.split(','):\n","        if (len(i) == 3) and (i not in cause_list):\n","          cause_list.append(i)\n","      if len(cause_list) < 1:\n","        return np.nan\n","      else:\n","        return cause_list\n","    elif m == \"\":\n","        return np.nan\n","    elif int(m) <= 9:\n","        result_lst = []\n","        result_lst.append(\"00\" + str(int(m)))\n","        return result_lst\n","    elif int(m) > 9:\n","        result_lst = []\n","        result_lst.append(\"0\" + str(int(m)))\n","        return result_lst\n","    else:\n","      return np.nan\n","  else:\n","     return np.nan\n","\n","df['causes'] = df['causes'].apply(lambda x: cause_list(x))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['causes'])"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-61-3903f216591b>:5: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n","  one_hot = pd.get_dummies(df['causes'].apply(pd.Series).stack(), prefix='', prefix_sep='').max(level=0)\n"]}],"source":["# replace np.nan values with 0 in the 'causes' feature\n","df['causes'] = df['causes'].fillna(0)\n","\n","# apply one-hot encoding on the 'causes' feature\n","one_hot = pd.get_dummies(df['causes'].apply(pd.Series).stack(), prefix='', prefix_sep='').max(level=0)\n","\n","# modify column names of one-hot encoded features\n","one_hot.columns = ['cause code ' + str(col) for col in one_hot.columns]\n","\n","# add the new one-hot encoded features to the original dataframe\n","df = pd.concat([df, one_hot], axis=1)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["# Filter rows where location is 'Foreign country'\n","foreign_country_rows = df[df['location'] == 'Ida-Virumaa']\n","\n","# Select the 'location' and 'temperature' columns\n","location_and_temperature = foreign_country_rows[['location', 'temperature']]"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["df = df.drop(columns='causes')"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["def citizenship(string):\n","    if not string:\n","        return '0'\n","    elif string.split(' -- ')[0].strip() == '':\n","        return '0'\n","    else:\n","        return string.split(' -- ')[0]\n","\n","df['citizenship'] = df['citizenship'].fillna('').apply(citizenship)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["df = df[df['citizenship'] != '0']"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['profession_code'])\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# Function to consolidate the sex codes\n","def profession_code(string):\n","    if type(string) != float: \n","        code = string.split(' -- ')[0]\n","        if code == '':\n","            return '00'\n","        else:\n","            return code[0:2]\n","    else:\n","        return np.nan\n","\n","\n","df['profession_code'] = df['profession_code'].apply(lambda x: profession_code(x))"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def add_general_profession_code(df1, df2):\n","    # replace non-numeric values in 'business_area' column with NaN\n","    df1['profession_code'] = pd.to_numeric(df1['profession_code'], errors='coerce')\n","    \n","    # create a dictionary mapping business area codes to general business codes\n","    code_map = df2.set_index('Kood')['Üldine_tähis'].to_dict()\n","    \n","    # add a new 'General_business_code' column to df1, initially filled with NaN\n","    df1['general_profession_class'] = pd.Series(dtype='object')  # change dtype to 'object' for string values\n","    \n","    # iterate over the rows in df1, checking if the business area code is in the dictionary\n","    for i, row in df1.iterrows():\n","        business_area_code = row['profession_code']\n","        if business_area_code in code_map:\n","            general_code = code_map[business_area_code]\n","        else:\n","            general_code = '00'  # if not found, set as 'missing'\n","        df1.at[i, 'general_profession_class'] = general_code\n","    \n","    return df1\n","\n","df_csv = pd.read_csv('ametikood.csv', sep=';')\n","df = add_general_profession_code(df, df_csv)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["df = df[df['general_profession_class'] != 99]"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["def injury_type(string):\n","    if isinstance(string, str):\n","        code = string.split(' -- ')[0]\n","        if len(code) == 3:\n","            if (code == '999') or (code == '000'):\n","                return '000'\n","            return code[0:2] + '0'\n","    return np.nan\n","  \n","df['type_of_injury'] = df['type_of_injury'].apply(lambda x: injury_type(x))"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['type_of_injury'])"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["df = df[df['type_of_injury'] != '000']"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["def injured_bodypart(string):\n","    if isinstance(string, str):\n","        code = string.split(' -- ')[0]\n","        if len(code) == 2:\n","            if (code == '99') or (code == '00'):\n","                return '00'\n","            return code[0:1] + '0'\n","    return np.nan\n","  \n","df['injured_bodypart'] = df['injured_bodypart'].apply(lambda x: injured_bodypart(x))"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["df = df[df['injured_bodypart'] != '00']"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["def workstation(string):\n","    code = string.split(' -- ')[0]\n","    if (int(code) >= 1) and (int(code) <= 8):\n","        return str(int(code))\n","    else:\n","        return '0'\n","\n","df['workstation'] = df['workstation'].apply(lambda x: workstation(x))"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["df = df[df['workstation'] != '0']"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["def working_environment(string):\n","    code = string.split(' -- ')[0]\n","    if (len(code) == 3):\n","        if (code == '999') or (code == '000'):\n","            return '000'\n","        else:\n","            return code[0:2] + '0'\n","    else:\n","        return np.nan\n","\n","df['working_environment'] = df['working_environment'].apply(lambda x: working_environment(x))"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['working_environment'])"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["df = df[df['working_environment'] != '000']"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["def working_process(string):\n","    code = string.split(' -- ')[0]\n","    if (int(code) == 0):\n","        return '00'\n","    elif (int(code) == 13):\n","        return '12'\n","    else:\n","        return code[0:1] + '0'\n","\n","df['working_process'] = df['working_process'].apply(lambda x: working_process(x))"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["df = df[df['working_process'] != '00']"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["df = df.dropna(subset=['deviation'])\n","df = df.dropna(subset=['contact_mode_of_injury'])\n","df = df.dropna(subset=['specific_physical_activity'])"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["def general(string):\n","    code = string.split(' -- ')[0]\n","    if (int(code) == 0):\n","        return '00'\n","    elif (int(code) == 99):\n","        return '00'\n","    else:\n","        return code[0:1] + '0'\n","\n","df['specific_physical_activity'] = df['specific_physical_activity'].apply(lambda x: general(x))\n","df['deviation'] = df['deviation'].apply(lambda x: general(x))\n","df['contact_mode_of_injury'] = df['contact_mode_of_injury'].apply(lambda x: general(x))"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["df = df[df['specific_physical_activity'] != '00']\n","df = df[df['deviation'] != '00']\n","df = df[df['contact_mode_of_injury'] != '00']"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["df = df.rename(columns={'material_agent_of_physical_act.': 'material_agent_of_physical_act'})"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["def agent(code):\n","  code_part1 = code.split(' -- ')[0]\n","  if len(code_part1) > 2:\n","    code_part2 = code_part1.split('.')\n","    if (code_part2[0] == '00'):\n","      return np.nan\n","    else:\n","      return code_part2[0] + '.' + '00'\n","  else:\n","    return 'other'\n","\n","df['material_agent_of_physical_act'] = df['material_agent_of_physical_act'].apply(lambda x: agent(x))\n","df['material_agent_of_deviation'] = df['material_agent_of_deviation'].apply(lambda x: agent(x))\n","df['material_agent_of_contact_mode'] = df['material_agent_of_contact_mode'].apply(lambda x: agent(x))"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["df = df[df['lost_days'] >= 0]"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["df.dropna(inplace=True)\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["df.to_csv('data.csv', encoding='latin-1')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ad77771add2c6521a9c58305fe4e03ccddeeb1fe739e6aac815f1fb97839e098"}}},"nbformat":4,"nbformat_minor":0}
